############################################################
# 2023


entry!('huang2023benchmarking',
  title('Benchmarking Large Language Models As {AI} Research Agents'),
  author('Qian Huang and Jian Vora and Percy Liang and J. Leskovec'),
  arxiv(2023),
  url('https://arxiv.org/pdf/2310.03302.pdf'),
)

entry!('yasunaga2023large',
  title('Large Language Models as Analogical Reasoners'),
  author('Michihiro Yasunaga and Xinyun Chen and Yujia Li and Panupong Pasupat and J. Leskovec and Percy Liang and Ed H. Chi and Denny Zhou'),
  arxiv(2023),
  url('https://arxiv.org/pdf/2310.01714.pdf'),
)

entry!('li2023benchmarking',
  title('Benchmarking and Improving Generator-Validator Consistency of Language Models'),
  author('Xiang Lisa Li and Vaishnavi Shrivastava and Siyan Li and Tatsunori Hashimoto and Percy Liang'),
  arxiv(2023),
  url('https://arxiv.org/pdf/2310.01846.pdf'),
)

entry!('fleming2023medalign',
  title('MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records'),
  author('S. Fleming and A. Lozano and W. Haberkorn and Jenelle A. Jindal and E. Reis and Rahul Thapa and L. Blankemeier and Julian Z. Genkins and E. Steinberg and A. Nayak and Birju S. Patel and Chia-Chun Chiang and A. Callahan and Zepeng Huo and S. Gatidis and S. Adams and Oluseyi Fayanju and Shreya J. Shah and Thomas Savage and Ethan Goh and A. Chaudhari and N. Aghaeepour and Christopher D. Sharp and M. Pfeffer and Percy Liang and Jonathan H. Chen and K. Morse and E. Brunskill and Jason Alan Fries and N. Shah'),
  arxiv(2023),
  url('https://arxiv.org/pdf/2308.14089.pdf'),
)

entry!('kuditipudi2023robust',
  title('Robust Distortion-free Watermarks for Language Models'),
  author('Rohith Kuditipudi and John Thickstun and Tatsunori Hashimoto and Percy Liang'),
  arxiv(2023),
  url('https://arxiv.org/pdf/2307.15593.pdf'),
)

entry!('liu2023lost',
  title('Lost in the Middle: How Language Models Use Long Contexts'),
  author('Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang'),
  arxiv(2023),
  url('https://arxiv.org/pdf/2307.03172.pdf'),
)

entry!('xie2023data',
  title('Data Selection for Language Models via Importance Resampling'),
  author('Sang Michael Xie and Shibani Santurkar and Tengyu Ma and Percy Liang'),
  arxiv(2023),
  url('https://arxiv.org/pdf/2302.03169.pdf'),
)

entry!('liu2023evaluating',
  title('Evaluating Verifiability in Generative Search Engines'),
  author('Nelson F. Liu and Tianyi Zhang and Percy Liang'),
  arxiv(2023),
  url('https://arxiv.org/pdf/2304.09848.pdf'),
)

entry!('lee2023evaluating',
  title('Evaluating Human-Language Model Interaction'),
  author('Mina Lee and Megha Srivastava and Amelia Hardy and John Thickstun and Esin Durmus and Ashwin Paranjape and Ines Gerard-Ursin and Xiang Lisa Li and Faisal Ladhak and Frieda Rong and Rose E. Wang and Minae Kwon and Joon Sung Park and Hancheng Cao and Tony Lee and Rishi Bommasani and Michael Bernstein and Percy Liang'),
  arxiv(2023, '2212.09746'),
  url('https://arxiv.org/pdf/2212.09746.pdf'),
)

entry!('bommasani2023trustworthy',
  title('Trustworthy Social Bias Measurement'),
  author('Rishi Bommasani and Percy Liang'),
  arxiv(2023),
  url('https://arxiv.org/pdf/2212.11672.pdf'),
)

entry!('zhang2023benchmarking',
  title('Benchmarking Large Language Models for News Summarization'),
  author('Tianyi Zhang and Faisal Ladhak and Esin Durmus and Percy Liang and K. McKeown and Tatsunori Hashimoto'),
  arxiv(2023),
  url('https://arxiv.org/pdf/2301.13848.pdf'),
)

entry!('khattab2023dsp',
  title('{Demonstrate-Search-Predict}: Composing retrieval and language models for knowledge-intensive {NLP}'),
  author('Omar Khattab and Keshav Santhanam and Xiang Lisa Li and David Hall and Percy Liang and Christopher Potts and Matei Zaharia'),
  arxiv(2023),
  url('https://arxiv.org/pdf/2212.14024.pdf'),
)

entry!('park2023generative',
  title('Generative Agents: Interactive Simulacra of Human Behavior'),
  author('Joon Sung Park and Joseph C. O\'Brien and Carrie J. Cai and Meredith Ringel Morris and Percy Liang and Michael S. Bernstein'),
  arxiv(2023),
  url('https://arxiv.org/pdf/2304.03442.pdf'),
)

entry!('henderson2023fairuse',
  title('Foundation Models and Fair Use'),
  author('Peter Henderson and Xuechen Li and Dan Jurafsky and Tatsunori Hashimoto and Mark A. Lemley and Percy Liang'),
  arxiv(2023),
  url('https://arxiv.org/pdf/2303.15715.pdf'),
)

entry!('bommasani2023ecosystem',
  title('Ecosystem Graphs: The Social Footprint of Foundation Models'),
  author('Rishi Bommasani and Dilara Soylu and Thomas Liao and Kathleen A. Creel and Percy Liang'),
  arxiv(2023),
  url('https://arxiv.org/pdf/2303.15772.pdf'),
)

# RSS 2023

entry!('karamcheti2023voltron',
  title('Language-driven Representation Learning for Robotics'),
  author('Siddharth Karamcheti and Suraj Nair and Annie S. Chen and T. Kollar and Chelsea Finn and Dorsa Sadigh and Percy Liang'),
  rss(2023),
  url('https://arxiv.org/pdf/2302.12766.pdf'),
  award('Best paper award finalist'),
)

# ICML 2023

entry!('sheng2023flexgen',
  title('High-throughput Generative Inference of Large Language Models with a Single {GPU}'),
  author('Ying Sheng and Lianmin Zheng and Binhang Yuan and Zhuohan Li and Max Ryabinin and Daniel Y. Fu and Zhiqiang Xie and Beidi Chen and Clark W. Barrett and Joseph Gonzalez and Percy Liang and Christopher Ré and I. Stoica and Ce Zhang'),
  icml(2023),
  url('https://arxiv.org/pdf/2303.06865.pdf'),
)

entry!('yasunaga2023retrieval',
  title('Retrieval-Augmented Multimodal Language Modeling'),
  author('Michihiro Yasunaga and Armen Aghajanyan and Weijia Shi and Rich James and Jure Leskovec and Percy Liang and Mike Lewis and Luke Zettlemoyer and Wen-tau Yih'),
  icml(2023),
  url('https://arxiv.org/pdf/2211.12561.pdf'),
)

entry!('santurkar2023opinions',
  title('Whose Opinions Do Language Models Reflect?'),
  author('Shibani Santurkar and Esin Durmus and Faisal Ladhak and Cinoo Lee and Percy Liang and Tatsunori Hashimoto'),
  icml(2023),
  url('https://arxiv.org/pdf/2303.17548.pdf'),
)

entry!('dubois2023evaluating',
  title('Evaluating Self-Supervised Learning via Risk Decomposition'),
  author('Yann Dubois and Tatsunori Hashimoto and Percy Liang'),
  icml(2023),
  url('https://arxiv.org/pdf/2302.03068.pdf'),
)

entry!('gao2023targeted',
  title('Out-of-Domain Robustness via Targeted Augmentations'),
  author('Irena Gao and Shiori Sagawa and Pang Wei Koh and Tatsunori Hashimoto and Percy Liang'),
  icml(2023),
  url('https://arxiv.org/pdf/2302.11861.pdf'),
)

# ACL 2023

entry!('liu2023sample',
  title('Are Sample-Efficient {NLP} Models More Robust?'),
  author('Nelson F. Liu and Ananya Kumar and Percy Liang and Robin Jia'),
  acl(2023),
  url('https://arxiv.org/abs/2210.06456.pdf'),
)

entry!('liu2023concurrence',
  title('Do Question Answering Modeling Improvements Hold Across Benchmarks?'),
  author('Nelson F. Liu and Tony Lee and Robin Jia and Percy Liang'),
  acl(2023),
  url('https://arxiv.org/pdf/2102.01065.pdf'),
)

entry!('zhang2023beyond',
  title('Beyond Positive Scaling: How Negation Impacts Scaling Trends of Language Models'),
  author('Yuhui Zhang* and Michihiro Yasunaga* and Zhengping Zhou* and Jeff Z. HaoChen* and James Zou and Percy Liang and Serena Yeung'),
  aclfindings(2023),
  url('https://arxiv.org/pdf/2305.17311.pdf'),
)

entry!('li2023contrastive',
  title('Contrastive Decoding: Open-ended Text Generation as Optimization'),
  author('Xiang Lisa Li and Ari Holtzman and Daniel Fried and Percy Liang and Jason Eisner and Tatsunori Hashimoto and Luke Zettlemoyer and M. Lewis'),
  acl(2023),
  url('https://arxiv.org/pdf/2210.15097.pdf'),
)

entry!('hewitt2023backpack',
  title('Backpack Language Models'),
  author('John Hewitt and John Thickstun and Christopher D. Manning and Percy Liang'),
  acl(2023),
  url('https://arxiv.org/pdf/2305.16765.pdf'),
  award('Outstanding paper award'),
)

# HRI 2023

entry!('cui2023corrections',
  title('"No, to the Right"-- Online Language Corrections for Robotic Manipulation via Shared Autonomy'),
  author('Yuchen Cui and Siddharth Karamcheti and Raj Palleti and Nidhya Shivakumar and Percy Liang and Dorsa Sadigh'),
  hri(2023),
  url('https://arxiv.org/pdf/2301.02555.pdf'),
)

# ICLR 2023

entry!('santurkar2023captions',
  title('Is a Caption Worth a Thousand Images? A Controlled Study for Representation Learning'),
  author('Shibani Santurkar and Yann Dubois and Rohan Taori and Percy Liang and Tatsunori Hashimoto'),
  iclr(2023),
  url('https://arxiv.org/pdf/2207.07635.pdf'),
)

entry!('lee2023surgical',
  title('Surgical Fine-Tuning Improves Adaptation to Distribution Shifts'),
  author('Yoonho Lee and Annie S. Chen and Fahim Tajwar and Ananya Kumar and Huaxiu Yao and Percy Liang and Chelsea Finn'),
  iclr(2023),
  url('https://arxiv.org/pdf/2210.11466.pdf'),
)

############################################################
# 2022

entry!('liang2022helm',
  title('Holistic Evaluation of Language Models'),
  author('Percy Liang and Rishi Bommasani and Tony Lee and Dimitris Tsipras and Dilara Soylu and Michihiro Yasunaga and Yian Zhang and D. Narayanan and Yuhuai Wu and Ananya Kumar and Benjamin Newman and Binhang Yuan and Bobby Yan and Ce Zhang and Christian Cosgrove and Christopher D. Manning and Christopher Ré and Diana Acosta-Navas and Drew A. Hudson and E. Zelikman and Esin Durmus and Faisal Ladhak and Frieda Rong and Hongyu Ren and Huaxiu Yao and Jue Wang and Keshav Santhanam and Laurel J. Orr and Lucia Zheng and Mert Yuksekgonul and Mirac Suzgun and Nathan S. Kim and Neel Guha and Niladri S. Chatterji and O. Khattab and Peter Henderson and Qian Huang and Ryan Chi and Sang Michael Xie and Shibani Santurkar and S. Ganguli and Tatsunori Hashimoto and Thomas F. Icard and Tianyi Zhang and Vishrav Chaudhary and William Wang and Xuechen Li and Yifan Mai and Yuhui Zhang and Yuta Koreeda'),
  arxiv(2022, '2211.09110'),
  url('https://arxiv.org/pdf/2211.09110.pdf'),
)

entry!('hewitt2022truncation',
  title('Truncation Sampling as Language Model Desmoothing'),
  author('John Hewitt and Christopher D. Manning and Percy Liang'),
  emnlpfindings(2022),
  url('https://arxiv.org/pdf/2210.15191.pdf'),
)

entry!('wu2022synthetic',
  title('Insights into Pre-training via Simpler Synthetic Tasks'),
  author('Yuhuai Wu and Felix Li and Percy Liang'),
  neurips(2022),
  url('https://arxiv.org/pdf/2206.10139.pdf'),
)

entry!('dubois2022issl',
  title('Improving Self-Supervised Learning by Characterizing Idealized Representations'),
  author('Yann Dubois and Tatsunori Hashimoto and S. Ermon and Percy Liang'),
  neurips(2022),
  url('https://arxiv.org/pdf/2209.06235.pdf'),
)

entry!('yuan2022decentralized',
  title('Decentralized Training of Foundation Models in Heterogeneous Environments'),
  author('Binhang Yuan and Yongjun He and Jared Quincy Davis and Tianyi Zhang and Tri Dao and Beidi Chen and Percy Liang and Christopher Re and Ce Zhang'),
  neurips(2022),
  url('https://arxiv.org/pdf/2206.01288.pdf'),
)

entry!('li2022diffusion',
  title('Diffusion-{LM} Improves Controllable Text Generation'),
  author('Xiang Lisa Li and John Thickstun and Ishaan Gulrajani and Percy Liang and Tatsunori Hashimoto'),
  neurips(2022),
  url('https://arxiv.org/pdf/2205.14217.pdf'),
)

entry!('garg2022incontext',
  title('What Can Transformers Learn In-Context? A Case Study of Simple Function Classes'),
  author('Shivam Garg and Dimitris Tsipras and Percy Liang and G. Valiant'),
  neurips(2022),
  url('https://arxiv.org/pdf/2208.01066.pdf'),
)

entry!('yasunaga2022dragon',
  title('Deep Bidirectional Language-Knowledge Graph Pretraining'),
  author('Michihiro Yasunaga and Antoine Bosselut and Hongyu Ren and Xikun Zhang and Christopher D. Manning and Percy Liang* and Jure Leskovec*'),
  neurips(2022),
  url('https://arxiv.org/pdf/2210.09338.pdf'),
)

entry!('bommasani2022homogenization',
  title('Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome Homogenization?'),
  author('Rishi Bommasani and Kathleen A. Creel and Ananya Kumar and Dan Jurafsky and Percy Liang'),
  neurips(2022),
  url('https://arxiv.org/pdf/2211.13972.pdf'),
)

entry!('donahue2022melody',
  title('Melody transcription via generative pre-training'),
  author('Chris Donahue and John Thickstun and Percy Liang'),
  ismir(2022),
  url('https://arxiv.org/pdf/2212.01884.pdf'),
)

entry!('park2022social',
  title('Social Simulacra: Creating Populated Prototypes for Social Computing Systems'),
  author('J. Park and Lindsay Popowski and Carrie J. Cai and M. Morris and Percy Liang and Michael S. Bernstein'),
  uist(2022),
  url('https://arxiv.org/pdf/2208.04024.pdf'),
)

entry!('wei2022emergent',
  title('Emergent Abilities of Large Language Models'),
  author('Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus'),
  tmlr(2022, 0),
  url('https://arxiv.org/pdf/2206.07682.pdf'),
)

entry!('kumar2022calibrated',
  title('Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift'),
  author('Ananya Kumar and Tengyu Ma and Percy Liang and Aditi Raghunathan'),
  uai(2022),
  url('https://arxiv.org/pdf/2207.08977.pdf'),
)

entry!('karamcheti2022lilac',
  title('Shared Autonomy for Robotic Manipulation with Language Corrections'),
  author('Siddharth Karamcheti* and Raj Palleti* and Yuchen Cui and Percy Liang and Dorsa Sadigh'),
  inproceedings('ACL Workshop for Learning with Natural Language Supervision (NL Supervision)', 2022),
  url('https://iliad.stanford.edu/pdfs/publications/karamcheti2022lilac.pdf'),
)

entry!('shen2022connect',
  title('Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised Domain Adaptation'),
  author('Kendrick Shen and Robbie Jones and Ananya Kumar and Sang Michael Xie and Jeff Z. HaoChen and Tengyu Ma and Percy Liang'),
  icml(2022),
  url('https://arxiv.org/pdf/2204.00570.pdf'),
)

entry!('yasunaga2022linkbert',
  title('{LinkBERT}: Pretraining Language Models with Document Links'),
  author('Michihiro Yasunaga and Jure Leskovec* and Percy Liang*'),
  acl(2022),
  url('https://arxiv.org/pdf/2203.15827.pdf'),
  codalab('0x7a6ab9c8d06a41d191335b270da2902e'),
)

entry!('sagawa2022uwilds',
  title('Extending the {WILDS} Benchmark for Unsupervised Adaptation'),
  author('Shiori Sagawa and Pang Wei Koh and Tony Lee and Irena Gao and Sang Michael Xie and Kendrick Shen and Ananya Kumar and Weihua Hu and Michihiro Yasunaga and H. Marklund and Sara Beery and E. David and I. Stavness and Wei Guo and J. Leskovec and Kate Saenko and Tatsunori B. Hashimoto and S. Levine and Chelsea Finn and Percy Liang'),
  iclr(2022),
  url('https://arxiv.org/pdf/2112.05090.pdf'),
)

entry!('kumar2022finetuning',
  title('Fine-tuning can Distort Pretrained Features and Underperform Out-of-Distribution'),
  author('Ananya Kumar and Aditi Raghunathan and Robbie Jones and Tengyu Ma and Percy Liang'),
  iclr(2022),
  codalab('0x40bd55abc9904fa3bd5eef53e0177a4c'),
  url('https://arxiv.org/pdf/2202.10054.pdf'),
)

entry!('xie2022incontext',
  title('An Explanation of In-context Learning as Implicit {B}ayesian Inference'),
  author('Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma'),
  iclr(2022),
  url('https://arxiv.org/pdf/2111.02080.pdf'),
  codalab('0xff6e1b45dc20429486bb91549a6e9660'),
)

entry!('li2022privacy',
  title('Large Language Models Can Be Strong Differentially Private Learners'),
  author('Xuechen Li and Florian Tramèr and Percy Liang and Tatsunori B. Hashimoto'),
  iclr(2022),
  url('https://arxiv.org/pdf/2110.05679.pdf'),
)

entry!('zhang2022greaselm',
  title('GreaseLM: Graph REASoning Enhanced Language Models for Question Answering'),
  author('Xikun Zhang and Antoine Bosselut and Michihiro Yasunaga and Hongyu Ren and Percy Liang and Christopher D. Manning and J. Leskovec'),
  iclr(2022),
  url('https://arxiv.org/pdf/2201.08860.pdf'),
)

entry!('lee2022coauthor',
  title('Co{A}uthor: Designing a Human-{AI} Collaborative Writing Dataset for Exploring Language Model Capabilities'),
  author('Mina Lee and Percy Liang and Qian Yang'),
  chi(2022),
  url('https://arxiv.org/pdf/2201.06796.pdf'),
  award('Honorable mention award'),
)

############################################################
# 2021

entry!('karamcheti2021lila',
  title('{LILA}: Language-Informed Latent Actions'),
  author('Siddharth Karamcheti* and Megha Srivastava* and Percy Liang and Dorsa Sadigh'),
  corl(2021),
  url('https://arxiv.org/pdf/2111.03205.pdf'),
)

entry!('bommasani2021opportunities',
  title('On the Opportunities and Risks of Foundation Models'),
  author('Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dorottya Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang'),
  arxiv(2021, '2108.07258'),
  url('https://arxiv.org/pdf/2108.07258.pdf'),
)

entry!('yasunaga2021language',
  title('{LM-Critic}: Language Models for Unsupervised Grammatical Error Correction'),
  author('Michihiro Yasunaga and Jure Leskovec and Percy Liang'),
  emnlp(2021),
  url('https://arxiv.org/pdf/2109.06822.pdf'),
  codalab('0x94456a63e1ee4ccfaabdc7f6a356cc82'),
)

entry!('hewitt2021conditional',
  title('Conditional probing: measuring usable information beyond a baseline'),
  author('John Hewitt and Kawin Ethayarajh and Percy Liang and Christopher D. Manning'),
  emnlp(2021),
  url('https://arxiv.org/pdf/2109.09234.pdf'),
  codalab('0x46190ef741004a43a2676a3b46ea0c76'),
)

entry!('castellon2021calm',
  title('Codified audio language modeling learns useful representations for music information retrieval'),
  author('Rodrigo Castellon and Chris Donahue and Percy Liang'),
  ismir(2021),
  url('https://arxiv.org/pdf/2107.05677.pdf'),
  codalab('0x7c5afa6f88bd4ff29fec75035332a583'),
  award('Best paper runner up'),
)

entry!('yasunaga2021break',
  title('{Break-It-Fix-It}: Unsupervised Learning for Program Repair'),
  author('Michihiro Yasunaga and Percy Liang'),
  icml(2021),
  url('https://arxiv.org/pdf/2106.06600.pdf'),
  codalab('0xfddb2ef01a9f4dc0b5d974a5a97174be'),
)

entry!('davis2021catformer',
  title('Catformer: Designing Stable Transformers via Sensitivity Analysis'),
  author('Jared Quincy Davis and Albert Gu and Krzysztof Choromanski and Tri Dao and Christopher Re and Chelsea Finn and Percy Liang'),
  icml(2021),
  url('http://proceedings.mlr.press/v139/davis21a/davis21a.pdf'),
)

entry!('liu2021jtt',
  title('Just Train Twice: Improving Group Robustness without Training Group Information'),
  author('Evan Zheran Liu and Behzad Haghgoo and Annie S. Chen and Aditi Raghunathan and Pang Wei Koh and Shiori Sagawa and Percy Liang and Chelsea Finn'),
  icml(2021),
  url('https://arxiv.org/pdf/2107.09044.pdf'),
)

entry!('liu2021dream',
  title('Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices'),
  author('Evan Zheran Liu and Aditi Raghunathan and Percy Liang and Chelsea Finn'),
  icml(2021),
  url('https://arxiv.org/pdf/2008.02790.pdf'),
)

entry!('xie2021composed',
  title('Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders for Improved Generalization'),
  author('Sang Michael Xie and Tengyu Ma and Percy Liang'),
  icml(2021),
  codalab('0x9187b7eee9d2453389bb63dfb45c4a89'),
  url('https://arxiv.org/pdf/2006.16205.pdf'),
)

entry!('koh2021wilds',
  title('{WILDS}: A Benchmark of in-the-Wild Distribution Shifts'),
  author('Pang Wei Koh* and Shiori Sagawa* and Henrik Marklund and Sang Michael Xie and Marvin Zhang and Akshay Balsubramani and Weihua Hu and Michihiro Yasunaga and Richard Lanas Phillips and Irena Gao and Tony Lee and Etienne David and Ian Stavness and Wei Guo and Berton A. Earnshaw and Imran S. Haque and Sara Beery and Jure Leskovec and Anshul Kundaje and Emma Pierson and Sergey Levine and Chelsea Finn and Percy Liang'),
  icml(2021),
  url('https://arxiv.org/pdf/2012.07421.pdf'),
)

entry!('miller2021line',
  title('Accuracy on the Line: on the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization'),
  author('John Miller and Rohan Taori and Aditi Raghunathan and Shiori Sagawa and Pang Wei Koh and Vaishaal Shankar and Percy Liang and Yair Carmon and Ludwig Schmidt'),
  icml(2021),
  url('https://arxiv.org/pdf/2107.04649.pdf'),
)

entry!('lee2021swords',
  title('Swords: A Benchmark for Lexical Substitution with Improved Data Coverage and Quality'),
  author('Mina Lee and C. Donahue and Robin Jia and Alexander Iyabor and Percy Liang'),
  naacl(2021),
  url('https://arxiv.org/pdf/2106.04102.pdf'),
)

entry!('yasunaga2021qagnn',
  title('{QA-GNN}: Reasoning with Language Models and Knowledge Graphs for Question Answering'),
  author('Michihiro Yasunaga and Hongyu Ren and Antoine Bosselut and Percy Liang and Jure Leskovec'),
  naacl(2021),
  url('https://arxiv.org/pdf/2104.06378.pdf'),
  codalab('0xf215deb05edf44a2ac353c711f52a25f'),
)

entry!('li2021prefix',
  title('Prefix-Tuning: Optimizing Continuous Prompts for Generation'),
  author('Xiang Lisa Li and Percy Liang'),
  acl(2021),
  url('https://arxiv.org/pdf/2101.00190.pdf'),
)

entry!('jones2021selective',
  title('Selective Classification Can Magnify Disparities Across Groups'),
  author('Erik Jones* and Shiori Sagawa* and Pang Wei Koh* and Ananya Kumar and Percy Liang'),
  iclr(2021),
  url('https://arxiv.org/pdf/2010.14134.pdf'),
  codalab('0x7ceb817d53b94b0c8294a7a22643bf5e'),
)

entry!('xie2021innout',
  title('In-{N}-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness'),
  author('Sang Michael Xie* and Ananya Kumar* and Robbie Jones* and Fereshte Khani and Tengyu Ma and Percy Liang'),
  iclr(2021),
  url('https://arxiv.org/pdf/2012.04550.pdf'),
  unusualCapitalization('N'),
  codalab('0x2613c72d4f3f4fbb94e0a32c17ce5fb0'),
)

entry!('khani2021removing',
  title('Removing Spurious Features can Hurt Accuracy and Affect Groups Disproportionately'),
  author('Fereshte Khani and Percy Liang'),
  facct(2021),
  url('https://arxiv.org/pdf/2012.04104.pdf'),
)

entry!('gu2021beyond',
  title('Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases'),
  author('Yu Gu and Sue Kase and Michelle T. Vanni and Brian M. Sadler and Percy Liang and Xifeng Yan and Yu Su'),
  www(2021),
  url('https://arxiv.org/pdf/2011.07743.pdf'),
)

entry!('koh2021stronger',
  title('Stronger Data Poisoning Attacks Break Data Sanitization Defenses'),
  author('Pang Wei Koh* and Jacob Steinhardt* and Percy Liang'),
  article('Machine Learning', 2021, 1),
  url('https://arxiv.org/pdf/1811.00741.pdf'),
)

############################################################
# 2020

entry!('karamcheti2020decomposition',
  title('Learning Adaptive Language Interfaces through Decomposition'),
  author('Siddharth Karamcheti and Dorsa Sadigh and Percy Liang'),
  inproceedings('EMNLP Workshop for Interactive and Executable Semantic Parsing (IntEx-SemPar)', 2020),
  url('https://arxiv.org/pdf/2010.05190.pdf'),
)

entry!('liu2020explore',
  title('Explore then Execute: Adapting without Rewards via Factorized Meta-Reinforcement Learning'),
  author('Evan Zheran Liu and Aditi Raghunathan and Percy Liang and Chelsea Finn'),
  arxiv(2020, '2008.02790'),
  url('https://arxiv.org/pdf/2008.02790.pdf'),
)

entry!('liu2020learning',
  title('Learning Abstract Models for Strategic Exploration and Fast Reward Transfer'),
  author('Evan Zheran Liu and Ramtin Keramati and Sudarshan Seshadri and Kelvin Guu and Panupong Pasupat and Emma Brunskill and Percy Liang'),
  arxiv(2020, '2007.05896'),
  url('https://arxiv.org/pdf/2007.05896.pdf'),
)

entry!('mussmann2020pairwise',
  title('On the Importance of Adaptive Data Collection for Extremely Imbalanced Pairwise Tasks'),
  author('Stephen Mussmann* and Robin Jia* and Percy Liang'),
  emnlpfindings(2020),
  url('https://arxiv.org/pdf/2010.05103.pdf'),
  codalab('0x39ba5559790b4099a7ff75f916ce19a4'),
)

entry!('hewitt2020rnn',
  title('{RNN}s can generate bounded hierarchical languages with optimal memory'),
  author('John Hewitt and Michael Hahn and Surya Ganguli and Percy Liang and Christopher D. Manning'),
  emnlp(2020),
  url('https://arxiv.org/pdf/2010.07515.pdf'),
  codalab('0xd668cf62e9e0499089626e45affee864'),
)

entry!('newman2020eos',
  author('Benjamin Newman and John Hewitt and Percy Liang and Christopher D. Manning'),
  title('The {EOS} Decision and Length Extrapolation'),
  inproceedings('Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP', 2020),
  url('https://arxiv.org/pdf/2010.07174.pdf'),
  award('Outstanding paper award'),
)

entry!('dathathri2020sdp',
  codalab('0x39ba5559790b4099a7ff75f916ce19a4'),
  title('Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming'),
  author('Sumanth Dathathri* and Krishnamurthy Dvijotham* and Alexey Kurakin* and Aditi Raghunathan* and Jonathan Uesato* and Rudy Bunel and Shreya Shankar and Jacob Steinhardt and Ian Goodfellow and Percy Liang and Pushmeet Kohli'),
  neurips(2020),
  url('https://arxiv.org/pdf/2010.11645.pdf'),
)

entry!('semanticmachines2020dataflow',
  title('Task-Oriented Dialogue as Dataflow Synthesis'),
  author('Semantic Machines and Jacob Andreas and John Bufe and David Burkett and Charles Chen and Josh Clausman and Jean Crawford and Kate Crim and Jordan DeLoach and Leah Dorner and Jason Eisner and Hao Fang and Alan Guo and David Hall and Kristin Hayes and Kellie Hill and Diana Ho and Wendy Iwaszuk and Smriti Jha and Dan Klein and Jayant Krishnamurthy and Theo Lanman and Percy Liang and Christopher H. Lin and Ilya Lintsbakh and Andy McGovern and Aleksandr Nisnevich and Adam Pauls and Dmitrij Petters and Brent Read and Dan Roth and Subhro Roy and Jesse Rusak and Beth Short and Div Slomin and Ben Snyder and Stephon Striplin and Yu Su and Zachary Tellman and Sam Thomson and Andrei Vorobev and Izabela Witoszko and Jason Wolfe and Abby Wray and Yuchen Zhang and Alexander Zotov'),
  tacl(2020, 8),
  url('https://arxiv.org/pdf/2009.11423.pdf'),
)

entry!('sagawa2020overparameterization',
  title('An investigation of why overparameterization exacerbates spurious correlations'),
  author('Shiori Sagawa* and Aditi Raghunathan* and Pang Wei Koh* and Percy Liang'),
  icml(2020),
  url('https://arxiv.org/pdf/2005.04345.pdf'),
)

entry!('koh2020bottleneck',
  title('Concept Bottleneck Models'),
  author('Pang Wei Koh* and Thao Nguyen* and Yew Siang Tang* and Stephen Mussmann and Emma Pierson and Been Kim and Percy Liang'),
  icml(2020),
  url('https://arxiv.org/pdf/2007.04612.pdf'),
  codalab('0x362911581fcd4e048ddfd84f47203fd2'),
)

entry!('khani2020noise',
  title('Feature Noise Induces Loss Discrepancy Across Groups'),
  author('Fereshte Khani and Percy Liang'),
  icml(2020),
  url('https://arxiv.org/pdf/1911.09876.pdf'),
  codalab('0x7c3fb3bf981646c9bc11c538e881f37e'),
)

entry!('yasunaga2020repair',
  title('Graph-based, Self-Supervised Program Repair from Diagnostic Feedback'),
  author('Michihiro Yasunaga and Percy Liang'),
  icml(2020),
  url('https://arxiv.org/pdf/2005.10636.pdf'),
  codalab('0x01838644724a433c932bef4cb5c42fbd'),
)

entry!('raghunathan2020understanding',
  title('Understanding and Mitigating the Tradeoff Between Robustness and Accuracy'),
  author('Aditi Raghunathan* and Sang Michael Xie* and Fanny Yang and John C. Duchi and Percy Liang'),
  icml(2020),
  url('https://arxiv.org/pdf/2002.10716.pdf'),
)

entry!('kumar2020gradual',
  title('Understanding Self-Training for Gradual Domain Adaptation'),
  author('Ananya Kumar and Tengyu Ma and Percy Liang'),
  icml(2020),
  url('https://arxiv.org/pdf/2002.11361.pdf'),
)

entry!('srivasta2020human',
  title('Robustness to Spurious Correlations via Human Annotations'),
  author('Megha Srivastava and Tatsunori Hashimoto and Percy Liang'),
  icml(2020),
  url('https://arxiv.org/pdf/2007.06661.pdf'),
  codalab('0xc8aa93946ffc48a69e59041144491fe1'),
)

entry!('jones2020roben',
  title('Robust Encodings: A Framework for Combating Adversarial Typos'),
  author('Erik Jones and Robin Jia* and Aditi Raghunathan* and Percy Liang'),
  acl(2020),
  url('https://arxiv.org/pdf/2005.01229.pdf'),
  codalab('0x8fc01c7fc2b742fdb29c05669f0ad7d2'),
)

entry!('kamath2020squads',
  title('Selective Question Answering under Domain Shift'),
  author('Amita Kamath and Robin Jia and Percy Liang'),
  acl(2020),
  url('https://arxiv.org/pdf/2006.09462.pdf'),
  codalab('0xea5a522788f743acb4fbf9e60065be8f'),
)

entry!('mu2020shaping',
  title('Shaping Visual Representations with Language for Few-shot Classification'),
  author('Jesse Mu and Percy Liang and Noah Goodman'),
  acl(2020),
  note('Short paper.'),
  url('https://arxiv.org/pdf/1911.02683.pdf'),
  codalab('0x55ed347e3beb4fa6971d3a226e48fc92'),
)

entry!('murty2020expbert',
  title('{ExpBERT}: Representation Engineering with Natural Language Explanations'),
  author('Shikhar Murty and Pang Wei Koh and Percy Liang'),
  acl(2020),
  note('Short paper.'),
  url('https://arxiv.org/pdf/2005.01932.pdf'),
  codalab('0x609d2d6a66194592a7f44fbb67ba9f49'),
)

entry!('donahue2020infilling',
  title('Enabling Language Models to Fill in the Blanks'),
  author('Chris Donahue and Mina Lee and Percy Liang'),
  acl(2020),
  note('Short paper.'),
  url('https://arxiv.org/pdf/2005.05339.pdf'),
)

entry!('sagawa2020group',
  title('Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization'),
  author('Shiori Sagawa* and Pang Wei Koh* and Tatsunori B. Hashimoto and Percy Liang'),
  iclr(2020),
  url('https://arxiv.org/pdf/1911.08731.pdf'),
)

entry!('hu2020pretraining',
  title('Strategies for Pre-training Graph Neural Networks'),
  author('Weihua Hu and Bowen Liu and Joseph Gomes and Marinka Zitnik and Percy Liang and Vijay Pande and Jure Leskovec'),
  iclr(2020),
  url('https://arxiv.org/pdf/1905.12265.pdf'),
)

entry!('coleman2020selection',
  title('Selection via Proxy: Efficient Data Selection for Deep Learning'),
  author('Cody Coleman and Christopher Yeh and Stephen Mussmann and Baharan Mirzasoleiman and Peter Bailis and Percy Liang and Jure Leskovec and Matei Zaharia'),
  iclr(2020),
  url('https://openreview.net/pdf?id=HJg2b0VYDr'),
)

entry!('li2020greedy',
  title('A Tight Analysis of Greedy Yields Subexponential Time Approximation for Uniform Decision Tree'),
  author('Ray Li and Percy Liang and Stephen Mussmann'),
  soda(2020),
  url('https://arxiv.org/pdf/1906.11385.pdf'),
)

############################################################
# 2019

entry!('jia2019certified',
  title('Certified Robustness to Adversarial Word Substitutions'),
  author('Robin Jia and Aditi Raghunathan and Kerem Göksel and Percy Liang'),
  emnlp(2019),
  url('https://arxiv.org/pdf/1909.00986.pdf'),
  codalab('0x79feda5f1998497db75422eca8fcd689'),
)

entry!('oren2019drolm',
  title('Distributionally Robust Language Modeling'),
  author('Yonatan Oren* and Shiori Sagawa* and Tatsunori Hashimoto* and Percy Liang'),
  emnlp(2019),
  url('https://arxiv.org/pdf/1909.02060.pdf'),
  codalab('0xf8122ebd24e94209a2a1764007509098'),
)

entry!('hewitt2019control',
  title('Designing and Interpreting Probes with Control Tasks'),
  author('John Hewitt and Percy Liang'),
  emnlp(2019),
  url('https://arxiv.org/pdf/1909.03368.pdf'),
  codalab('0xb0c351d6f1ac4c51b54f1023786bf6b2'),
  award('Best paper runner up'),
)

entry!('kulal2019spoc',
  title('SPoC: Search-based Pseudocode to Code'),
  author('Sumith Kulal and Panupong Pasupat and Kartik Chandra and Mina Lee and Oded Padon and Alex Aiken and Percy Liang'),
  neurips(2019),
  url('https://arxiv.org/pdf/1906.04908.pdf'),
)

entry!('carmon2019unlabeled',
  title('Unlabeled Data Improves Adversarial Robustness'),
  author('Yair Carmon* and Aditi Raghunathan* and Ludwig Schmidt and Percy Liang and John C. Duchi'),
  neurips(2019),
  url('https://arxiv.org/pdf/1905.13736.pdf'),
  codalab('0x9df253b24dac4a2b930108be9c6e5496'),
)

entry!('kumar2019calibration',
  title('Verified Uncertainty Calibration'),
  author('Ananya Kumar and Percy Liang and Tengyu Ma'),
  neurips(2019),
  url('https://arxiv.org/pdf/1909.10155.pdf'),
  codalab('0xb6d027ee127e422989ab9115726c5411'),
)

entry!('koh2019influence',
  title('On the Accuracy of Influence Functions for Measuring Group Effects'),
  author('Pang Wei Koh* and Kai-Siang Ang* and Hubert H. K. Teo* and Percy Liang'),
  neurips(2019),
  url('https://arxiv.org/pdf/1905.13289.pdf'),
  codalab('0x43e8ecea53bf4186a7992c9028409320'),
)

entry!('lee2019autocomplete',
  title('Learning Autocomplete Systems as a Communication Game'),
  author('Mina Lee and Tatsunori Hashimoto and Percy Liang'),
  inproceedings('Emergent Communication Workshop at Neural Information Processing Systems (NeurIPS)', 2019),
  url('https://arxiv.org/pdf/1911.06964.pdf'),
  codalab('0x627dc6dc62874a45ba8abc6ada10e753'),
)

entry!('raghunathan2019hurt',
  title('Adversarial Training Can Hurt Generalization'),
  author('Aditi Raghunathan* and Sang Michael Xie* and Fanny Yang and John C. Duchi and Percy Liang'),
  arxiv(2019, '1906.06032'),
  url('https://arxiv.org/pdf/1906.06032.pdf'),
)

entry!('khani2019mwld',
  title('Maximum Weighted Loss Discrepancy'),
  author('Fereshte Khani and Aditi Raghunathan and Percy Liang'),
  arxiv(2019, '1906.03518'),
  url('https://arxiv.org/pdf/1906.03518.pdf'),
  codalab('0x578f01269d644524b0d4ab2a7a2a6984'),
)

entry!('monajemi2019painless',
  title('Ambitious Data Science Can Be Painless'),
  author('Hatef Monajemi and Riccardo Murri and Eric Jonas and Percy Liang and Victoria Stodden and David L. Donoho'),
  article('Harvard Data Science Review', 2019, 1),
  url('https://arxiv.org/pdf/1901.08705.pdf'),
)

entry!('hashimoto2019huse',
  title('Unifying Human and Statistical Evaluation for Natural Language Generation'),
  author('Tatsunori Hashimoto* and Hugh Zhang* and Percy Liang'),
  naacl(2019),
  url('https://arxiv.org/pdf/1904.02792.pdf'),
  codalab('0x88644b5ee189402eb19d39d721d1005c'),
)

entry!('peng2019pun',
  title('Pun Generation with Surprise'),
  author('Nanyun Peng* and He He* and Percy Liang'),
  naacl(2019),
  url('https://arxiv.org/pdf/1904.06828.pdf'),
  codalab('0x5a7d0fe35b144ad68998d74891a31ed6'),
)

entry!('selsam2019sat',
  title('Learning a {SAT} Solver from Single-Bit Supervision'),
  author('Daniel Selsam and Matthew Lamm and Benedikt Bünz and Percy Liang and Leonardo de Moura and David L. Dill'),
  iclr(2019),
  url('https://arxiv.org/pdf/1802.03685.pdf'),
)

entry!('zhang2019discretization',
  title('Defending against Whitebox Adversarial Attacks via Randomized Discretization'),
  author('Yuchen Zhang and Percy Liang'),
  aistats(2019),
  url('https://arxiv.org/pdf/1903.10586.pdf'),
  codalab('0x822ba2f9005f49f08755a84443c76456'),
)

entry!('pierson2019aging',
  title('Inferring Multidimensional Rates of Aging from Cross-Sectional Data'),
  author('Emma Pierson and Pang Wei Koh and Tatsunori Hashimoto and Daphne Koller and Jure Leskovec and Nick Eriksson and Percy Liang'),
  aistats(2019),
  url('https://arxiv.org/pdf/1807.04709.pdf'),
)

entry!('shi2019frangel',
  title('{F}r{A}ngel: Component-Based Synthesis with Control Structures'),
  author('Kensen Shi and Jacob Steinhardt and Percy Liang'),
  popl(2019),
  url('https://arxiv.org/pdf/1811.05175.pdf'),
  codalab('0x882075c0b92c4a2d85abdbd3d76aad78'),
)

############################################################
# 2018

entry!('raghunathan2018sdp',
  title('Semidefinite relaxations for certifying robustness to adversarial examples'),
  author('Aditi Raghunathan and Jacob Steinhardt and Percy Liang'),
  neurips(2018),
  url('https://arxiv.org/pdf/1811.01057.pdf'),
)

entry!('mussmann2018sgd',
  title('Uncertainty Sampling is Preconditioned Stochastic Gradient Descent on Zero-One Loss'),
  author('Stephen Mussmann and Percy Liang'),
  neurips(2018),
  codalab('0xf8dfe5bcc1dc408fb54b3cc15a5abce8'),
  url('https://arxiv.org/pdf/1812.01815.pdf'),
)

entry!('hashimoto2018edit',
  title('A Retrieve-and-Edit Framework for Predicting Structured Outputs'),
  author('Tatsunori Hashimoto and Kelvin Guu and Yonatan Oren and Percy Liang'),
  neurips(2018),
  url('https://arxiv.org/pdf/1812.01194.pdf'),
  codalab('0x1ad3f387005c492ea913cf0f20c9bb89'),
)

entry!('choi2018quac',
  title('{QuAC}: Question Answering in Context'),
  author('Eunsol Choi and He He and Mohit Iyyer and Mark Yatskar and Wen-tau Yih and Yejin Choi and Percy Liang and Luke Zettlemoyer'),
  url('https://arxiv.org/pdf/1808.07036.pdf'),
  emnlp(2018),
)

entry!('he2018negotiation',
  title('Decoupling Strategy and Generation in Negotiation Dialogues'),
  author('He He and Derek Chen and Anusha Balakrishnan and Percy Liang'),
  emnlp(2018),
  url('https://arxiv.org/pdf/1808.09637.pdf'),
  codalab('0x453913e76b65495d8b9730d41c7e0a0c'),
)

entry!('pasupat2018elements',
  title('Mapping Natural Language Commands to Web Elements'),
  author('Panupong Pasupat and Tian-Shun Jiang and Evan Zheran Liu and Kelvin Guu and Percy Liang'),
  emnlp(2018),
  url('https://arxiv.org/pdf/1808.09132.pdf'),
  note('Short paper.'),
  codalab('0x0097f249cd944284a81af331093c3579'),
)

entry!('lamm2018tap',
  title('Textual Analogy Parsing: What\'s Shared and What\'s Compared among Analogous Facts'),
  author('Matthew Lamm and Arun Chaganty and Christopher D. Manning and Dan Jurafsky and Percy Liang'),
  emnlp(2018),
  url('https://arxiv.org/pdf/1809.02700.pdf'),
)

entry!('mussmann2018accuracy',
  title('On the Relationship between Data Efficiency and Error in Active Learning'),
  author('Stephen Mussmann and Percy Liang'),
  icml(2018),
  url('https://arxiv.org/pdf/1806.06123.pdf'),
  codalab('0x8ef22fd3cd384029bf1d1cae5b268f2d'),
)

entry!('hashimoto2018repeated',
  title('Fairness Without Demographics in Repeated Loss Minimization'),
  author('Tatsunori B. Hashimoto and Megha Srivastava and Hongseok Namkoong and Percy Liang'),
  icml(2018),
  url('https://arxiv.org/pdf/1806.08010.pdf'),
  codalab('0x17a501d37bbe49279b0c70ae10813f4c'),
  award('Best paper runner up'),
)

entry!('hancock2018babble',
  title('Training Classifiers with Natural Language Explanations'),
  author('Braden Hancock and Paroma Varma and Stephanie Wang and Martin Bringmann and Percy Liang and Christopher R\\\'e'),
  acl(2018),
  url('https://arxiv.org/pdf/1805.03818.pdf'),
  codalab('0x900e7e41deaa4ec5b2fe41dc50594548'),
)

entry!('chaganty2018evaluation',
  title('The price of debiasing automatic metrics in natural language evaluation'),
  author('Arun Chaganty and Stephen Mussmann and Percy Liang'),
  acl(2018),
  codalab('0xbda93e6519134c1ab1893ceaa19c8a5c'),
  url('https://arxiv.org/pdf/1807.02202.pdf'),
)

entry!('rajpurkar2018squadrun',
  title('Know What You Don\'t Know: Unanswerable Questions for {SQuAD}'),
  author('Pranav Rajpurkar and Robin Jia and Percy Liang'),
  acl(2018),
  award('Best short paper award'),
  url('https://arxiv.org/pdf/1806.03822.pdf'),
  codalab('0x9a15a170809f4e2cb7940e1f256dee55'),
)

entry!('mussmann2018gbs',
  title('Generalized Binary Search For Split-Neighborly Problems'),
  author('Stephen Mussmann and Percy Liang'),
  aistats(2018),
  url('https://arxiv.org/pdf/1802.09751.pdf'),
)

entry!('khani2018pip',
  title('Planning, Inference and Pragmatics in Sequential Language Games'),
  author('Fereshte Khani and Noah D. Goodman and Percy Liang'),
  tacl(2018, 6),
  url('https://arxiv.org/pdf/1805.11774.pdf'),
  codalab('0x052129c7afa9498481185b553d23f0f9'),
)

entry!('guu2018edit',
  author('Kelvin Guu and Tatsunori B. Hashimoto and Yonatan Oren and Percy Liang'),
  title('Generating Sentences by Editing Prototypes'),
  tacl(2018, 0),
  url('http://arxiv.org/pdf/1709.08878.pdf'),
  codalab('0xa915ba2f8b664ddf8537c83bde80cc8c'),
)

entry!('li2018style',
  author('Juncen Li and Robin Jia and He He and Percy Liang'),
  title('Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer'),
  naacl(2018),
  url('https://arxiv.org/pdf/1804.06437.pdf'),
  codalab('0xe3eb416773ed4883bb737662b31b4948'),
)

entry!('liu2018workflow',
  author('Evan Zheran Liu and Kelvin Guu and Panupong Pasupat and Tianlin Shi and Percy Liang'),
  title('Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration'),
  iclr(2018),
  url('https://arxiv.org/pdf/1802.08802.pdf'),
)

entry!('raghunathan2018certified',
  author('Aditi Raghunathan and Jacob Steinhardt and Percy Liang'),
  title('Certified defenses against adversarial examples'),
  iclr(2018),
  url('https://arxiv.org/pdf/1801.09344.pdf'),
  codalab('0xa21e794020bb474d8804ec7bc0543f52'),
)

entry!('bastani2018active',
  author('Osbert Bastani and Rahul Sharma and Alex Aiken and Percy Liang'),
  title('Active Learning of Points-To Specifications'),
  pldi(2018),
  url('https://arxiv.org/pdf/1711.03239.pdf'),
)

entry!('sharan2018prediction',
  author('Vatsal Sharan and Sham Kakade and Percy Liang and Gregory Valiant'),
  title('Prediction with a Short Memory'),
  stoc(2018),
  url('http://arxiv.org/pdf/1612.02526.pdf'),
)

entry!('demszky2018transforming',
  author('Dorottya Demszky and Kelvin Guu and Percy Liang'),
  title('Transforming Question Answering Datasets Into Natural Language Inference Datasets'),
  arxiv(2018, '1809.02922'),
)

############################################################
# 2017

entry!('steinhardt2017certified',
  author('Jacob Steinhardt and Pang Wei Koh and Percy Liang'),
  title('Certified Defenses for Data Poisoning Attacks'),
  neurips(2017),
  url('http://arxiv.org/pdf/1706.03691.pdf'),
  codalab('0xbdd35bdd83b14f6287b24c9418983617'),
)

entry!('hashimoto2017transformation',
  author('Tatsunori B. Hashimoto and John Duchi and Percy Liang'),
  title('Unsupervised Transformation Learning via Convex Relaxations'),
  neurips(2017),
  url('https://arxiv.org/pdf/1711.02226.pdf'),
  codalab('0x4412d72b55df47e5b457dacdba91f105'),
)

entry!('sharan2017overcomplete',
  author('Vatsal Sharan and Sham Kakade and Percy Liang and Gregory Valiant'),
  title('Learning Overcomplete {HMM}s'),
  neurips(2017),
  url('https://arxiv.org/pdf/1711.02309.pdf'),
)

entry!('jia2017adversarial',
  author('Robin Jia and Percy Liang'),
  title('Adversarial Examples for Evaluating Reading Comprehension Systems'),
  emnlp(2017),
  codalab('0xc86d3ebe69a3427d91f9aaa63f7d1e7d'),
  award('Outstanding paper award'),
  url('http://arxiv.org/pdf/1707.07328.pdf'),
)

entry!('zhang2017macro',
  author('Yuchen Zhang and Panupong Pasupat and Percy Liang'),
  title('Macro Grammars and Holistic Triggering for Efficient Semantic Parsing'),
  emnlp(2017),
  codalab('0x4d6dbfc5ec7f44a6a4da4ca2a9334d6e'),
  url('https://zhangyuc.github.io/files/zhang17emnlp.pdf'),
)

entry!('chaganty2017unbiased',
  author('Arun Chaganty and Ashwin Paranjape and Percy Liang and Chris Manning'),
  title('Importance sampling for unbiased on-demand evaluation of knowledge base population'),
  url('https://nlp.stanford.edu/pubs/chaganty2017ondemand.pdf'),
  emnlp(2017),
)

entry!('koh2017understanding',
  author('Pang Wei Koh and Percy Liang'),
  title('Understanding Black-box Predictions via Influence Functions'),
  award('Best paper award'),
  icml(2017),
  url('http://arxiv.org/pdf/1703.04730.pdf'),
  codalab('0x2b314dc3536b482dbba02783a24719fd'),
)

entry!('zhang2017convexified',
  author('Yuchen Zhang and Percy Liang and Martin J. Wainwright'),
  title('Convexified Convolutional Neural Networks'),
  icml(2017),
  url('http://arxiv.org/pdf/1609.01000.pdf'),
)

entry!('selsam2017bugfree',
  author('Daniel Selsam and Percy Liang and David Dill'),
  title('Developing Bug-Free Machine Learning Systems With Formal Mathematics'),
  icml(2017),
  url('https://arxiv.org/pdf/1706.08605.pdf'),
  code('https://github.com/dselsam/certigrad'),
)

entry!('shi2017wob',
  author('Tianlin Shi and Andrej Karpathy and Linxi Fan and Jonathan Hernandez and Percy Liang'),
  title('World of Bits: An Open-Domain Platform for Web-Based Agents'),
  icml(2017),
  url('http://proceedings.mlr.press/v70/shi17a/shi17a.pdf'),
)

entry!('zhang2017hitting',
  author('Yuchen Zhang and Percy Liang and Moses Charikar'),
  title('A Hitting Time Analysis of Stochastic Gradient {L}angevin Dynamics'),
  colt(2017),
  url('http://arxiv.org/pdf/1702.05575.pdf'),
  award('Best paper award'),
)

entry!('bastani2017synthesizing',
  author('Osbert Bastani and Rahul Sharma and Alex Aiken and Percy Liang'),
  title('Synthesizing Program Input Grammars'),
  pldi(2017),
  url('http://arxiv.org/pdf/1608.01723.pdf'),
)

entry!('wang2017naturalizing',
  author('Sida I. Wang and Sam Ginn and Percy Liang and Christopher D. Manning'),
  title('Naturalizing a Programming Language via Interactive Learning'),
  acl(2017),
  project('http://www.voxelurn.com'),
  url('https://arxiv.org/pdf/1704.06956.pdf'),
  codalab('0xbf8f4f5b42e54eba9921f7654b3c5c5d'),
)

entry!('he2017symmetric',
  author('He He and Anusha Balakrishnan and Mihail Eric and Percy Liang'),
  title('Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings'),
  acl(2017),
  url('https://arxiv.org/pdf/1704.07130.pdf'),
  codalab('0xc757f29f5c794e5eb7bfa8ca9c945573'),
  pages(1766,1776),
)

entry!('guu2017bridging',
  author('Kelvin Guu and Panupong Pasupat and Evan Zheran Liu and Percy Liang'),
  title('From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood'),
  acl(2017),
  url('https://arxiv.org/pdf/1704.07926.pdf'),
  codalab('0x88c914ee1d4b4a4587a07f36f090f3e5'),
)

############################################################
# 2016

entry!('steinhardt2016risk',
  author('Jacob Steinhardt and Percy Liang'),
  title('Unsupervised Risk Estimation Using Only Conditional Independence Structure'),
  neurips(2016),
  url('https://arxiv.org/pdf/1606.05313.pdf'),
)

entry!('rajpurkar2016squad',
  author('Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang'),
  title('{SQuAD}: 100,000+ Questions for Machine Comprehension of Text'),
  emnlp(2016),
  award('Best resource paper award'),
  url('https://arxiv.org/pdf/1606.05250.pdf'),
  codalab('0xd53d03a48ef64b329c16b9baf0f99b0c'),
)

entry!('wang2016games',
  author('Sida I. Wang and Percy Liang and Chris Manning'),
  title('Learning Language Games through Interaction'),
  acl(2016),
  award('Outstanding paper award'),
  url('https://arxiv.org/pdf/1606.02447.pdf'),
  codalab('0x9fe4d080bac944e9a6bd58478cb05e5e'),
)

entry!('jia2016recombination',
  author('Robin Jia and Percy Liang'),
  title('Data Recombination for Neural Semantic Parsing'),
  acl(2016),
  url('http://arxiv.org/pdf/1606.03622.pdf'),
  codalab('0x50757a37779b485f89012e4ba03b6f4f'),
)

entry!('long2016projections',
  author('Reginald Long and Panupong Pasupat and Percy Liang'),
  title('Simpler Context-Dependent Logical Forms via Model Projections'),
  acl(2016),
  url('https://arxiv.org/pdf/1606.05378.pdf'),
  codalab('0xad3fc9f52f514e849b282a105b1e3f02'),
)

entry!('pasupat2016inferring',
  author('Panupong Pasupat and Percy Liang'),
  title('Inferring Logical Forms From Denotations'),
  acl(2016),
  url('https://arxiv.org/pdf/1606.06900.pdf'),
  codalab('0x47cc64d9c8ba4a878807c7c35bb22a42'),
)

entry!('khani2016unanimity',
  author('Fereshte Khani and Martin Rinard and Percy Liang'),
  title('Unanimous Prediction for 100\% Precision with Application to Learning Semantic Mappings'),
  acl(2016),
  url('unanimity-acl2016.pdf'),
  codalab('0x593676a278fc4e5abe2d8bac1e3df486'),
)

entry!('chaganty2016perspectives',
  author('Arun Tejasvi Chaganty and Percy Liang'),
  title('How Much is 131 Million Dollars? {P}utting Numbers in Perspective with Compositional Descriptions'),
  acl(2016),
  url('perspectives-acl2016.pdf'),
  codalab('0x243284b4d81d4590b46030cdd3b72633'),
)

entry!('raghunathan2016linear',
  author('Aditi Raghunathan and Roy Frostig and John Duchi and Percy Liang'),
  title('Estimation from Indirect Supervision with Linear Moments'),
  icml(2016),
  url('http://arxiv.org/pdf/1608.03100.pdf'),
  codalab('0x6a264a96efea41158847eef9ec2f76bc'),
)

entry!('wager2016levy',
  title('Data Augmentation via {L}\\\'evy Processes'),
  author('Stefan Wager and Will Fithian and Percy Liang'),
  incollection('Perturbations, Optimization and Statistics', 2016),
  url('http://arxiv.org/pdf/1603.06340.pdf'),
  code('https://github.com/swager/levythin'),
)

entry!('liang2016executable',
  title('Learning Executable Semantic Parsers for Natural Language Understanding'),
  article('Communications of the ACM', 2016, 59),
  url('executable-cacm2016.pdf'),
  author('Percy Liang'),
)

############################################################
# 2015

entry!('wang2015overnight',
  acl(2015),
  title('Building a Semantic Parser Overnight'),
  author('Yushi Wang and Jonathan Berant and Percy Liang'),
  project('http://www-nlp.stanford.edu/software/sempre/'),
  url('overnight-acl2015.pdf'),
  codalab('0x269ef752f8c344a28383240f7bb2be9c'),
  tags('semantic parsing'),
)

entry!('berant2015agenda',
  tacl(2015, 3),
  title('Imitation Learning of Agenda-Based Semantic Parsers'),
  author('Jonathan Berant and Percy Liang'),
  pages(545, 558),
  url('http://nlp.stanford.edu/pubs/berant-liang-tacl2015.pdf'),
  project('http://www-nlp.stanford.edu/software/sempre/'),
  codalab('0x8fdfad310dd84b7baf683b520b4b64d5'),
)

entry!('steinhardt2015relaxed',
  neurips(2015),
  title('Learning with Relaxed Supervision'),
  author('Jacob Steinhardt and Percy Liang'),
  url('relaxed-nips2015.pdf'),
  codalab('0xc9db508bb80446d2b66cbc8e2c74c052'),
)

entry!('wang2015polynomial',
  neurips(2015),
  title('Estimating Mixture Models via Mixture of Polynomials'),
  author('Sida I. Wang and Arun Chaganty and Percy Liang'),
  url('polynomial-nips2015.pdf'),
  codalab('0xca42b883b1f9481989cfb02fe693649f'),
)

entry!('werling2015onthejob',
  neurips(2015),
  title('On-the-Job Learning with {B}ayesian Decision Theory'),
  author('Keenon Werling and Arun Chaganty and Percy Liang and Chris Manning'),
  url('onthejob-nips2015.pdf'),
  codalab('0x2ae89944846444539c2d08a0b7ff3f6f'),
)

entry!('kuleshov2015calibrated',
  neurips(2015),
  title('Calibrated Structured Prediction'),
  author('Volodymyr Kuleshov and Percy Liang'),
  url('calibration-nips2015.pdf'),
  codalab('0xecc9a01cfcbc4cd6b0444a92d259a87c'),
)

entry!('kuleshov2015simultaneous',
  arxiv(2015),
  extendedVersion,
  author('Volodymyr Kuleshov and Arun Chaganty and Percy Liang'),
  title('Simultaneous diagonalization: the asymmetric, low-rank, and noisy settings'),
  url('http://arxiv.org/pdf/1501.06318.4182.pdf'),
)

entry!('guu2015traversing',
  emnlp(2015),
  title('Traversing Knowledge Graphs in Vector Space'),
  author('Kelvin Guu and John Miller and Percy Liang'),
  award('Best paper honorable mention'),
  url('http://arxiv.org/pdf/1506.01094.pdf'),
  codalab('0xfcace41fdeec45f3bc6ddf31107b829f'),
)

entry!('pasupat2015compositional',
  acl(2015),
  title('Compositional Semantic Parsing on Semi-Structured Tables'),
  author('Panupong Pasupat and Percy Liang'),
  url('compositional-acl2015.pdf'),
  codalab('0xf26cd79d4d734287868923ad1067cf4c'),
)

entry!('misra2015environment',
  acl(2015),
  title('Environment-Driven Lexicon Induction for High-Level Instructions'),
  author('Dipendra K. Misra and Kejia Tao and Percy Liang and Ashutosh Saxena'),
  url('environment-acl2015.pdf'),
  codalab('0x7f9151ec074f4f589e4d4786db7bb6de'),
)

entry!('steinhardt2015rcm',
  author('Jacob Steinhardt and Percy Liang'),
  title('Reified Context Models'),
  url('http://arxiv.org/pdf/1502.06665.pdf'),
  icml(2015),
  codalab('0x8967960a7c644492974871ee60198401'),
)

entry!('steinhardt2015fast',
  author('Jacob Steinhardt and Percy Liang'),
  title('Learning Fast-Mixing Models for Structured Prediction'),
  url('http://arxiv.org/pdf/1502.06668.pdf'),
  icml(2015), pages(1063, 1072),
  codalab('0xc6edf0c9bec643ac9e74418bd6ad4136'),
)

entry!('shi2015sample',
  aistats(2015),
  author('Tianlin Shi and Jacob Steinhardt and Percy Liang'),
  title('Learning Where To Sample in Structured Prediction'),
  url('sample-aistats2015.pdf'),
  codalab('0x66df55eda5054cbf9e173520c7b6ac3d'),
  pages(875, 884),
)

entry!('kuleshov2015tensor',
  aistats(2015),
  author('Volodymyr Kuleshov and Arun Chaganty and Percy Liang'),
  title('Tensor factorization via matrix factorization'),
  url('http://arxiv.org/pdf/1501.07320.pdf'),
  codalab('0x56dc93bcd3a647b197ad6e4b9d56f336'),
)

entry!('liang2015semantics',
  author('Percy Liang and Christopher Potts'),
  title('Bringing machine learning and compositional semantics together'),
  article('Annual Reviews of Linguistics', 2015, 1, 1),
  pages(355, 376),
  url('http://www.stanford.edu/~cgpotts/manuscripts/liang-potts-semantics.pdf'),
  tags('semantic parsing'),
)

############################################################
# 2014

entry!('steinhardt2014sparse',
  arxiv(2014, '1412.4182'),
  author('Jacob Steinhardt and Stefan Wager and Percy Liang'),
  title('The Statistics of Streaming Sparse Regression'),
  url('http://arxiv.org/pdf/1412.4182.pdf'),
)

entry!('ramanathan2014linking',
  eccv(2014),
  title('Linking people with "their" names using coreference resolution'),
  author('Vignesh Ramanathan and Armand Joulin and Percy Liang and Li Fei-Fei'),
  url('linking-eccv2014.pdf'),
  supplementalurl('linking-eccv2014-supp.pdf'),
)

entry!('liang2014talking',
  article('XRDS: Crossroads, The ACM Magazine for Students', 2014, 21),
  title('Talking to computers in natural language'),
  author('Percy Liang'),
  number(1),
  pages(18, 21),
  publisher('ACM'),
  url('talking-xrds2014.pdf'),
)

entry!('berant2014paraphrasing',
  author('Jonathan Berant and Percy Liang'),
  title('Semantic Parsing via Paraphrasing'),
  acl(2014),
  url('http://aclweb.org/anthology/P14-1133'),
  award('Best long paper honorable mention'),
  project('http://www-nlp.stanford.edu/software/sempre/'),
  tags('semantic parsing'),
)

entry!('pasupat2014extraction',
  author('Panupong Pasupat and Percy Liang'),
  title('Zero-shot Entity Extraction from Web Pages'),
  acl(2014),
  url('extraction-acl2014.pdf'),
  slidesurl('extraction-acl2014-talk.pdf'),
  project('http://www-nlp.stanford.edu/software/web-entity-extractor-ACL2014/'),
)

entry!('chaganty2014graphical',
  author('Arun Chaganty and Percy Liang'),
  title('Estimating Latent-Variable Graphical Models using Moments and Likelihoods'),
  url('graphical-icml2014.pdf'),
  slidesurl('graphical-icml2014-talk.pdf'),
  icml(2014),
  tags('spectral'),
)

entry!('steinhardt2014eg',
  author('Jacob Steinhardt and Percy Liang'),
  title('Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm'),
  url('eg-icml2014.pdf'),
  icml(2014),
)

entry!('steinhardt2014filtering',
  author('Jacob Steinhardt and Percy Liang'),
  title('Filtering with Abstract Particles'),
  icml(2014),
  pages(727, 735),
  url('filtering-icml2014.pdf'),
  supplementalurl('filtering-icml2014-appendix.pdf'),
)

entry!('frostig2014subconstant',
  author('Roy Frostig and Sida I. Wang'),
  title('A sub-constant improvement in approximating the positive semidefinite {G}rothendieck problem'),
  arxiv(2014, '1408.2270'),
  url('http://arxiv.org/abs/1408.2270'),
)

entry!('wager2014altitude',
  title('Altitude Training: Strong Bounds for Single-Layer Dropout'),
  author('Stefan Wager and Will Fithian and Sida I. Wang and Percy Liang'),
  neurips(2014),
  url('http://arxiv.org/pdf/1407.3289.pdf'),
)

entry!('frostig2014lowrank',
  title('Simple {MAP} inference via low-rank relaxations'),
  author('Roy Frostig and Sida I. Wang and Percy Liang and Chris Manning'),
  neurips(2014),
  url('http://cs.stanford.edu/~rfrostig/pubs/lowrank-nips2014.pdf'),
  codalab('0x106abb3b47be492aa7387f528c943faa'),
)

entry!('wang2014iqp',
  author('Sida I. Wang and Roy Frostig and Percy Liang and Chris Manning'),
  title('Relaxations for inference in restricted {B}oltzmann machines'),
  iclrWorkshop(2014),
  url('http://arxiv.org/abs/1312.6205'),
)

############################################################
# 2013

entry!('liang2013lambdadcs',
  author('Percy Liang'),
  title('Lambda Dependency-Based Compositional Semantics'),
  arxiv(2013, '1309.4408'),
  url('http://arxiv.org/pdf/1309.4408.pdf'),
)

entry!('berant2013freebase',
  author('Jonathan Berant and Andrew Chou and Roy Frostig and Percy Liang'),
  title('Semantic Parsing on {F}reebase from Question-Answer Pairs'),
  emnlp(2013),
  url('http://www.aclweb.org/anthology/D13-1160'),
  slidesurl('http://cs.stanford.edu/~pliang/papers/freebase-emnlp2013-talk.pdf'),
  supplementalurl('http://arxiv.org/pdf/1309.4408.pdf'),
  project('http://www-nlp.stanford.edu/software/sempre/'),
  tags('semantic parsing'),
)

entry!('wang2013noising',
  author('Sida I. Wang and Mengqiu Wang and Stefan Wager and Percy Liang and Chris Manning'),
  title('Feature Noising for Log-linear Structured Prediction'),
  emnlp(2013),
  url('noising-emnlp2013.pdf'),
)

entry!('wager2013dropout',
  author('Stefan Wager and Sida I. Wang and Percy Liang'),
  title('Dropout Training as Adaptive Regularization'),
  neurips(2013),
  url('http://arxiv.org/pdf/1307.1493.pdf'),
  posterurl('dropout-nips2013-poster.pdf'),
)

entry!('chaganty13regression',
  author('Arun Chaganty and Percy Liang'),
  title('Spectral Experts for Estimating Mixtures of Linear Regressions'),
  icml(2013),
  url('http://arxiv.org/pdf/1306.3729.pdf'),
  tags('spectral'),
)

entry!('ramanathan2013event',
  author('Vignesh Ramanathan and Percy Liang and Li Fei-Fei'),
  title('Video Event Understanding using Natural Language Descriptions'),
  iccv(2013),
  url('event-iccv2013.pdf'),
)

entry!('sharma13algebraic',
  author('Rahul Sharma and Saurabh Gupta and Bharath Hariharan and Alex Aiken and Percy Liang and Aditya V. Nori'),
  title('A Data Driven Approach for Algebraic Loop Invariants'),
  inproceedings('European Symposium on Programming (ESOP)', 2013),
  url('algebraic-esop2013.pdf'),
)

############################################################
# 2012 and earlier

entry!('hsu12identifiability',
  author('Daniel Hsu and Sham M. Kakade and Percy Liang'),
  title('Identifiability and Unmixing of Latent Parse Trees'),
  neurips(2012),
  abstract('This paper explores unsupervised learning of parsing models along two directions. First, which models are identifiable from infinite data? We use a general technique for numerically checking identifiability based on the rank of a Jacobian matrix, and apply it to several standard constituency and dependency parsing models. Second, for identifiable models, how do we estimate the parameters efficiently? EM suffers from local optima, while recent work using spectral methods cannot be directly applied since the topology of the parse tree varies across sentences. We develop a strategy, unmixing, which deals with this additional complexity for restricted classes of parsing models.'),
  url('identifiability-nips2012.pdf'),
  posterurl('identifiability-nips2012-poster.pdf'),
  techreporturl('http://arxiv.org/pdf/1206.3137.pdf'),
  tags('spectral'),
)

entry!('liang13cl',
  author('Percy Liang and Michael Jordan and Dan Klein'),
  title('Learning Dependency-Based Compositional Semantics'),
  computationalLinguistics(2013, 39), pages(389, 446),
  extendedVersion,
)

entry!('liang11thesis',
  author('Percy Liang'),
  title('Learning Dependency-Based Compositional Semantics'),
  phdthesis('University of California Berkeley at Berkeley', 2011),
  extendedVersion,
)

entry!('liang11dcs',
  author('Percy Liang and Michael I. Jordan and Dan Klein'),
  title('Learning Dependency-Based Compositional Semantics'),
  acl2011,
  pages(590, 599),
  url('dcs-acl2011.pdf'),
  slidesurl('dcs-acl2011-talk.pdf'),
  thesisurl('dcs-thesis2011.pdf'),
  journalurl('dcs-cl2012.pdf'),
  code('../software/dcs.zip'),
  abstract('Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.  In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.  In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.  On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite using less supervision than existing systems.'),
  punchlines('Task: learn to map questions to answers via latent logical forms.',
             'Contribution: new tree-based semantic representation.',
             'Result: surpass state-of-the-art on semantic parsing with less supervision.'),
)

entry!('liang11pruning',
  author('Percy Liang and Mayur Naik'),
  title('Scaling up Abstraction Refinement via Pruning'),
  pldi(2011),
  url('pruning-pldi2011.pdf'),
  slidesurl('pruning-pldi2011-talk.pdf'),
  abstract('Many static analyses do not scale as they are made more precise.  For example, increasing the amount of context sensitivity in a k-limited pointer analysis causes the number of contexts to grow exponentially with k.  Iterative refinement techniques can mitigate this growth by starting with a coarse abstraction and only refining parts of the abstraction that are deemed relevant with respect to a given client.',
           'In this paper, we introduce a new technique called pruning that uses client feedback in a different way.  The basic idea is to use coarse abstractions to prune away parts of the program analysis deemed irrelevant for proving a client query, and then using finer abstractions on the sliced program analysis.  For a k-limited pointer analysis, this approach amounts to adaptively refining and pruning a set of prefix patterns representing the contexts relevant for the client.  By pruning, we are able to scale up to much more expensive abstractions than before.  We also prove that the pruned analysis is both sound and complete, that is, it yields the same results as an analysis that uses a more expensive abstraction directly without pruning.'),
  punchlines('Idea: run cheap analysis, use client feedback to prune away irrelvant parts of program analysis (think program slicing); then run expensive analysis.',
             'Theoretical result: pruning is sound and complete.',
             'Empirical result: we can use much richer $k$-object-sensitivity abstractions.'),
)

entry!('liang11minimal',
  author('Percy Liang and Omer Tripp and Mayur Naik'),
  title('Learning Minimal Abstractions'),
  popl(2011),
  url('minimal-popl2011.pdf'),
  slidesurl('minimal-popl2011-talk.pdf'),
  abstract('Static analyses are generally parametrized by an abstraction which is chosen from a family of abstractions.  We are interested in flexible families of abstractions with many parameters, as these families can allow one to increase precision in ways tailored to the client without sacrificing scalability.  For example, we consider k-limited points-to analyses where each call site and allocation site in a program can have a different k value.  We then ask a natural question in this paper: What is the minimal (coarsest) abstraction in a given family which is able to prove a set of queries?  In addressing this question, we make the following two contributions: (i) We introduce two machine learning algorithms for efficiently finding a minimal abstraction; and (ii) for a static race detector backed by a k-limited points-to analysis, we show empirically that minimal abstractions are actually quite coarse: It suffices to provide context/object sensitivity to a very small fraction (0.4--2.3%) of the sites to yield equally precise results as providing context/object sensitivity uniformly to all sites.'),
  punchlines('Question: how small is the smallest abstraction needed to prove a query?',
             'Empirical answer: very small (less than 2.5% sites need to be treated context-sensitively for k-limited analyses for race detection).',
             'Found this answer using a new machine learning algorithm that exploits this sparsity.'),
)

entry!('golland2010pragmatics',
  emnlp(2010),
  author('Dave Golland and Percy Liang and Dan Klein'),
  title('A Game-theoretic Approach to Generating Spatial Descriptions'),
  pages(410, 419),
  url('pragmatics-emnlp2010.pdf'),
  slidesurl('pragmatics-emnlp2010-talk.pdf'),
)

entry!('angeli10generation',
  author('Gabor Angeli and Percy Liang and Dan Klein'),
  title('A Simple Domain-Independent Probabilistic Approach to Generation'),
  emnlp(2010),
  url('generation-emnlp2010.pdf'),
  slidesurl('generation-emnlp2010-talk.pdf'),
  abstract('We present a simple, robust generation system which performs content selection and surface realization in a unified, domain-independent framework.  In our approach, we break up the end-to-end generation process into a sequence of local decisions, arranged hierarchically and each trained discriminatively.  We deployed our system in three different domains---Robocup sportscasting, technical weather forecasts, and common weather forecasts, obtaining results comparable to state-of-the-art domain-specific systems both in terms of BLEU scores and human evaluation.'),
  punchlines(
    'Model natural language generation as a sequence of local decisions, each backed by a log-linear model.',
    'Advantage: can use arbitrary expressive features, works across multiple domains.'),
)

entry!('liang10abstraction',
  author('Percy Liang and Omer Tripp and Mayur Naik and Mooly Sagiv'),
  title('A Dynamic Evaluation of Static Heap Abstractions'),
  oopsla(2010),
  url('abstractions-oopsla2010.pdf'),
  slidesurl('abstractions-oopsla2010-talk.pdf'),
  abstract('The quality of a static analysis of heap-manipulating programs is largely determined by its heap abstraction.  Object allocation sites are a commonly-used abstraction, but are too coarse for some clients.  The goal of this paper is to investigate how various refinements of allocation sites can improve precision.  In particular, we consider abstractions that use call stack, object recency, and heap connectivity information.  We measure the precision of these abstractions dynamically for four different clients motivated by concurrency and on nine Java programs chosen from the DaCapo benchmark suite.  Our dynamic results shed new light on aspects of heap abstractions that matter for precision, which allows us to more effectively navigate the large space of possible heap abstractions.'),
  punchlines('Question: what aspects of a heap abstraction matter?',
             'Methodology: run program (9 DaCapo benchmarks) dynamically, compute static heap abstractions (3 dimensions of refinement: context sensitivity, object recency, and shape analysis), answer client queries (4 clients based on concurrency).'),
)

entry!('liang10programs',
  author('Percy Liang and Michael I. Jordan and Dan Klein'),
  title('Learning Programs: A Hierarchical {B}ayesian Approach'),
  icml2010,
  pages(639, 646),
  url('programs-icml2010.pdf'),
  slidesurl('programs-icml2010-talk.pdf'),
  code('../software/program-induction.zip'),
  abstract('We are interested in learning programs for multiple related tasks given only a few training examples per task.  Since the program for a single task is underdetermined by its data, we introduce a nonparametric hierarchical Bayesian prior over programs which shares statistical strength across multiple tasks.  The key challenge is to parametrize this multi-task sharing.  For this, we introduce a new representation of programs based on combinatory logic and provide an MCMC algorithm that can perform safe program transformations on this representation to reveal shared inter-program substructures.'),
  punchlines('Programs are trees, subprograms are subtrees, which can be shared across tasks.  Combinators refactor programs to expose the appropriate subprograms.'),
)

entry!('liang10regimes',
  author('Percy Liang and Nati Srebro'),
  title('On the Interaction between Norm and Dimensionality: Multiple Regimes in Learning'),
  icml2010,
  url('regimes-icml2010.pdf'),
  slidesurl('regimes-icml2010-talk.pdf'),
  abstract('A learning problem might have several measures of complexity (e.g., norm and dimensionality) that affect the generalization error.  What is the interaction between these complexities?  Dimension-free learning theory bounds and parametric asymptotic analyses each provide a partial picture of the full learning curve.  In this paper, we use high-dimensional asymptotics on two classical problems---mean estimation and linear regression---to explore the learning curve more completely.  We show that these curves exhibit multiple regimes, where in each regime, the excess risk is controlled by a subset of the problem complexities.'),
  punchlines('Goal: understand excess risk as a function of sample size and problem complexity.  On simple examples, show that asymptotic risk has multiple regimes, each controlled by different complexities.'),
)

entry!('liang10type',
  author('Percy Liang and Michael I. Jordan and Dan Klein'),
  title('Type-Based {MCMC}'),
  naacl2010,
  url('type-naacl2010.pdf'),
  slidesurl('type-naacl2010-talk.pdf'),
  code('../software/typesampling.zip'),
  abstract('Most existing algorithms for learning latent-variable models---such as EM and existing Gibbs samplers---are token-based, meaning that they update the variables associated with one sentence at a time.  The incremental nature of these methods makes them susceptible to local optima/slow mixing.  In this paper, we introduce a type-based sampler, which updates a block of variables, identified by a type, which spans multiple sentences.  We show improvements on part-of-speech induction, word segmentation, and learning tree-substitution grammars.'),
  punchlines('NLP perspective: goal is to avoid local optima by processing all tokens associated with a type at once instead of one token or sentence at a time.',
             'Sampling perspective: new type of block sampling that exploits exchangeability.'),
)

entry!('liang10regularizationTR',
  author('Percy Liang and Francis Bach and Guillaume Bouchard and Michael I. Jordan'),
  title('Asymptotically Optimal Regularization in Smooth Parametric Models'),
  arxiv(2010),
  extendedVersion,
  abstract('Many types of regularization schemes have been employed in statistical learning, each one motivated by some assumption about the problem domain.  In this paper, we present a unified asymptotic analysis of smooth regularizers, which allows us to see how the validity of these assumptions impacts the success of a particular regularizer.  In addition, our analysis motivates an algorithm for optimizing regularization parameters, which in turn can be analyzed within our framework.  We apply our analysis to several examples, including hybrid generative-discriminative learning and multi-task learning.'),
  punchlines('Setting: estimator defined by minimizing loss plus regularization.',
             'Question: what is the best regularizer to use?',
             'This is hard to optimize, so use a Taylor expansion instead.'),

)
entry!('liang09regularization',
  author('Percy Liang and Francis Bach and Guillaume Bouchard and Michael I. Jordan'),
  title('Asymptotically Optimal Regularization in Smooth Parametric Models'),
  neurips(2009),
  url('regularization-nips2009.pdf'),
  techreporturl('regularization-techreport2010.pdf'),
  posterurl('regularization-nips2009-poster.pdf'),
  abstract('Many types of regularization schemes have been employed in statistical learning, each one motivated by some assumption about the problem domain.  In this paper, we present a unified asymptotic analysis of smooth regularizers, which allows us to see how the validity of these assumptions impacts the success of a particular regularizer.  In addition, our analysis motivates an algorithm for optimizing regularization parameters, which in turn can be analyzed within our framework.  We apply our analysis to several examples, including hybrid generative-discriminative learning and multi-task learning.'),
  punchlines('Setting: estimator defined by minimizing loss plus regularization.',
             'Question: what is the best regularizer to use?',
             'This is hard to optimize, so use a Taylor expansion instead, yielding a interpretable closed form solution.'),
)

entry!('liang09hdppcfg',
  author('Percy Liang and Michael I. Jordan and Dan Klein'),
  title('Probabilistic grammars and hierarchical {D}irichlet processes'),
  incollection('The Oxford Handbook of Applied Bayesian Analysis', 2009),
  editor('T. O\'Hagan and M. West'),
  publisher('Oxford University Press'),
  url('hdppcfg-haba.pdf'),
  abstract('Probabilistic context-free grammars (PCFGs) have played an important role in the modeling of syntax in natural language processing and other applications, but choosing the proper model complexity is often difficult.  We present a nonparametric Bayesian generalization of the PCFG based on the hierarchical Dirichlet process (HDP).  In our HDP-PCFG model, the effective complexity of the grammar can grow with increasing data.  We describe an efficient variational inference algorithm for our model and present experiments on both a synthetic grammar induction task and a large-scale natural language parsing task.'),
  punchlines('Details of the EMNLP 2007 paper + general background, empirical intuitions, and derivations for structured mean-field + a small grammar induction experiment.'),
)

entry!('liang09semantics',
  author('Percy Liang and Michael I. Jordan and Dan Klein'),
  title('Learning Semantic Correspondences with Less Supervision'),
  aclijcnlp2009,
  pages(91, 99),
  url('semantics-acl2009.pdf'),
  slidesurl('semantics-acl2009-talk.pdf'),
  code('../software/unsupervised-modeling.zip'),
  abstract('A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state.  To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state.  We show that our model generalizes across three domains of increasing difficulty---Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.'),
  punchlines('Stuff happens in the world.  A text talks about it.  Our goal: learn the correspondence between the two.',
             'Approach: probabilistic model capturing identification of entities/events in the world, segmentation of the text, and alignment between the two.'),
)

entry!('liang09measurements',
  author('Percy Liang and Michael I. Jordan and Dan Klein'),
  title('Learning from Measurements in Exponential Families'),
  icml2009,
  url('measurements-icml2009.pdf'),
  slidesurl('measurements-icml2009-talk.pdf'),
  abstract('Given a model family and a set of unlabeled examples, one could either label specific examples or state general constraints---both provide information about the desired model.  In general, what is the most cost-effective way to learn?  To address this question, we introduce measurements, a general class of mechanisms for providing information about a target model. We present a Bayesian decision-theoretic framework, which allows us to both integrate diverse measurements and choose new measurements to make.  We use a variational inference algorithm, which exploits exponential family duality. The merits of our approach are demonstrated on two sequence labeling tasks.'),
  punchlines('Goal: learning with minimum human effort.',
             'Things human can do: label data, provide constraints---in general, make measurements.',
             'Use Bayesian decision theory to choose optimal measurements.'),
)

entry!('liang09online',
  author('Percy Liang and Dan Klein'),
  title('Online {EM} for Unsupervised Models'),
  naacl2009,
  pages(611, 619),
  url('online-naacl2009.pdf'),
  slidesurl('online-naacl2009-talk.pdf'),
  code('../software/unsupervised-modeling.zip'),
  abstract('The (batch) EM algorithm plays an important role in unsupervised induction, but it sometimes suffers from slow convergence.  In this paper, we show that online variants (1) provide significant speedups and (2) can even find better solutions than those found by batch EM.  We support these findings on four unsupervised tasks: part-of-speech tagging, document classification, word segmentation, and word alignment.'),
  punchlines('What you\'d expect: online is faster than batch.',
             'What you might not expect: online gets better accuarcy than batch.'),
)

entry!('liang08asymptotics',
  author('Percy Liang and Michael I. Jordan'),
  title('An Asymptotic Analysis of Generative, Discriminative, and Pseudolikelihood Estimators'),
  icml2008, pages(584, 591),
  url('asymptotics-icml2008.pdf'),
  slidesurl('asymptotics-icml2008-talk.pdf'),
  award('Best student paper award'),
  abstract('Statistical and computational concerns have motivated parameter estimators based on various forms of likelihood, e.g., joint, conditional, and pseudolikelihood.  In this paper, we present a unified framework for studying these estimators, which allows us to compare their relative (statistical) efficiencies.  Our asymptotic analysis suggests that modeling more of the data tends to reduce variance, but at the cost of being more sensitive to model misspecification.  We present experiments validating our analysis.'),
  punchlines('Derive general expression for the asymptotic risk of composite likelihood estimators in exponential families.',
             'This allows us to compare the various estimators.'),
)

entry!('liang08structure',
  author('Percy Liang and Hal {Daum{\\\'e} III} and Dan Klein'),
  title('Structure Compilation: Trading Structure for Features'),
  icml2008,
  url('structure-icml2008.pdf'),
  slidesurl('structure-icml2008-talk.pdf'),
  abstract('Structured models often achieve excellent performance but can be slow at test time.  We investigate structure compilation, where we replace structure with features, which are often computationally simpler but unfortunately statistically more complex.  We analyze this tradeoff theoretically and empirically on three natural language processing tasks.  We also introduce a simple method to transfer predictive power from structure to features via unlabeled data, while incurring a minimal statistical penalty.'),
  punchlines('How much do we lose by throwing out edge features in CRFs and adding node features?',
             'Studies the approximation, estimation, computational aspects of the tradeoff.'),
)

entry!('liang08errors',
  author('Percy Liang and Dan Klein'),
  title('Analyzing the Errors of Unsupervised Learning'),
  hltacl2008,
  url('errors-acl2008.pdf'),
  slidesurl('errors-acl2008-talk.pdf'),
  abstract('We identify four types of errors that unsupervised induction systems make and study each one in turn.  Our contributions include (1) using a meta-model to analyze the incorrect biases of a model in a systematic way, (2) providing an efficient and robust method of measuring distance between two parameter settings of a model, and (3) showing that local optima issues which typically plague EM can be somewhat alleviated by increasing the number of training examples.  We conduct our analyses on three models: the HMM, the PCFG, and a simple dependency model.'),
  punchlines('Error decomposition: approximation, identifiability, estimation, optimization errors.',
             'Used meta-model to analyze approximation error.',
             'Empirically observed that more data reduces optimization error.'),
)

entry!('haghighi08lexicon',
  author('Aria Haghighi and Percy Liang and Taylor Berg-Kirkpatrick and Dan Klein'),
  title('Learning Bilingual Lexicons from Monolingual Corpora'),
  hltacl2008,
  url('lexicon-acl2008.pdf'),
  code('../software/unsuplex.zip'),
  abstract('We present a method for learning bilingual translation lexicons from monolingual corpora.  Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings.  Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings. We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types.'),
  punchlines('By using CCA, can do word alignment without the usual sentence-aligned corpora.'),
)

entry!('liang08agreement',
  author('Percy Liang and Dan Klein and Michael I. Jordan'),
  title('Agreement-Based Learning'),
  neurips(2008),
  url('agreement-nips2008.pdf'),
  posterurl('agreement-nips2008-poster.pdf'),
  abstract(<<EOF
The learning of probabilistic models with many hidden variables
and non-decomposable dependencies is an important and challenging problem.
In contrast to traditional approaches based on approximate inference in a single
intractable model, our approach is to train a set of tractable
submodels by encouraging them to agree on the hidden variables.  This allows
us to capture non-decomposable aspects of the data while still maintaining
tractability.  We propose an objective function for our approach,
derive EM-style algorithms for parameter estimation, and demonstrate their
effectiveness on three challenging real-world learning tasks.
EOF
  ),
  punchlines('Setting: unsupervised learning.',
             'Alternative to approximate inference: make two tractable models and train them to agree.',
             'Advantage: maintain existing tractable inference procedures as black-boxes.'),
)

entry!('bouchard08language',
  author("Alexandre Bouchard-C\\^ot\\'e and Percy Liang and Tom Griffiths and Dan Klein"),
  title('A Probabilistic Approach to Language Change'),
  neurips(2008),
  url('language-nips2008.pdf'),
  posterurl('language-nips2008-poster.pdf'),
  abstract(<<EOF
We present a probabilistic approach to language change in which word forms are
represented by phoneme sequences that undergo stochastic edits along the
branches of a phylogenetic tree. This framework combines the
advantages of the classical comparative method with the robustness
of corpus-based probabilistic models.  We use this framework to
explore the consequences of two different schemes for defining
probabilistic models of phonological change, evaluating these
schemes by reconstructing ancient word forms of Romance languages.
The result is an efficient inference procedure for automatically
inferring ancient word forms from modern languages, which can be
generalized to support inferences about linguistic phylogenies.
EOF
  ),
  punchlines('Feature-based generative model of phonemes of words in a phylogeny of languages.'),
)

entry!('liang07tutorial',
  author('Percy Liang and Dan Klein'),
  title('Structured {B}ayesian Nonparametric Models with Variational Inference (tutorial)'),
  acl2007,
  url('tutorial-acl2007.pdf'),
  slidesurl('tutorial-acl2007-talk.pdf'),
)

entry!('liang07permdp',
  author('Percy Liang and Michael I. Jordan and Ben Taskar'),
  title('A permutation-augmented sampler for {D}irichlet process mixture models'),
  icml2007,
  punchlines('Task: clustering.',
            'Idea: conditioned on a permutation of the data points, '+
            'one can consider all possible clusterings of those data points '+
            'which are consistent with the permutation '+
            'using dynamic programming.'),
  abstract(<<EOF
We introduce a new inference algorithm for Dirichlet process mixture
models.  While Gibbs sampling and variational methods focus on local
moves, the new algorithm makes more global moves.  This is done by
introducing a permutation of the data points as an auxiliary variable.
The algorithm is a blocked sampler which alternates between sampling the
clustering and sampling the permutation.  The key to the efficiency of
this approach is that it is possible to use dynamic programming to
consider all exponentially many clusterings consistent with a given
permutation.  We also show that random projections can be used to
effectively sample the permutation.  The result is a stochastic
hill-climbing algorithm that yields burn-in times significantly
smaller than those of collapsed Gibbs sampling.
EOF
  ),
  url('permutation-dp-icml2007.pdf'),
  slidesurl('permutation-dp-icml2007-talk.pdf'),
  punchlines('Idea: if data are ordered, can cluster using dynamic programming.',
             'Let this ordering be a random auxiliary variable and we get a sampler.'),
)

entry!('liang07infpcfg',
  author('Percy Liang and Slav Petrov and Michael I. Jordan and Dan Klein'),
  title('The Infinite {PCFG} using Hierarchical {D}irichlet Processes'),
  emnlpconll2007,
  url('hdppcfg-emnlp2007.pdf'),
  slidesurl('hdppcfg-emnlp2007-talk.pdf'),
  abstract(<<EOF
We present a nonparametric Bayesian model of tree structures based on the
hierarchical Dirichlet process (HDP).  Our HDP-PCFG model allows the complexity
of the grammar to grow as more training data is available.  In addition to
presenting a fully Bayesian model for the PCFG, we also develop an efficient
variational inference procedure.  On synthetic data, we recover the correct
grammar without having to specify its complexity in advance.  We also show that
our techniques can be applied to full-scale parsing applications by
demonstrating its effectiveness in learning state-split grammars.
EOF
  ),
  punchlines(
    'A PCFG with an infinite number of states.',
    'Learning: variational inference.'),
)

entry!('bouchard07diachronic',
  author("Alexandre Bouchard-C\\^ot\\'e and Percy Liang and Tom Griffiths and Dan Klein"),
  title('A Probabilistic Approach to Diachronic Phonology'),
  emnlpconll2007,
  url('diachronic-emnlp2007.pdf'),
  abstract(<<EOF
We present a probabilistic model of diachronic phonology in which individual
word forms undergo stochastic edits along the branches of a phylogenetic tree.
Our approach allows us to achieve three goals with a single unified
model: (1) reconstruction of both ancient and modern word forms, (2) discovery
of general phonological changes, and (3) selection among different
phylogenies.  We learn our model using a Monte Carlo EM algorithm and present
quantitative results validating the model.
EOF
  ),
  punchlines('Generative model of phonemes of words in a phylogeny of languages'),
)

entry!('liang06discrimative',
  author("Percy Liang and Alexandre Bouchard-C\\^ot\\'e and Dan Klein and Ben Taskar"),
  title('An End-to-End Discriminative Approach to Machine Translation'),
  colingacl2006,
  url('discriminative-mt-acl2006.pdf'),
  slidesurl('discriminative-mt-acl2006-talk.pdf'),
  punchlines('Task: machine translation.',
            'Idea: treat machine translation as a structured classification task '+
            '(learn a map from input sentence to output sentence). '+
            'Use a Perceptron-like algorithm: '+
            'decode and update towards maximum BLEU scoring translation on the n-best list.'),
  abstract(<<EOF
We present a perceptron-style discriminative approach to machine
translation in which large feature sets can be exploited.  Unlike
discriminative reranking approaches, our system can take advantage of learned
features in all stages of decoding.  We first discuss several challenges to
error-driven discriminative approaches.  In particular, we explore different
ways of updating parameters given a training example.  We find that making
frequent but smaller updates is preferable to making fewer but larger updates.
Then, we discuss an array of features and show both how they quantitatively
increase BLEU score and how they qualitatively interact on specific examples.
One particular feature we investigate is a novel way to introduce learning into
the initial phrase extraction process, which has previously been entirely
heuristic.
EOF
  ),
)

entry!('liang06alignment',
  author('Percy Liang and Ben Taskar and Dan Klein'),
  title('Alignment by Agreement'),
  hltnaacl2006, pages(104, 111),
  url('alignment-naacl2006.pdf'),
  slidesurl('alignment-naacl2006-talk.pdf'),
  code('../software/cross-em-aligner.zip'),
  punchlines('Task: unsupervised word alignment.',
            'Idea: Jointly train two HMM models (one in each direction) '+
            'to encourage agreement. '+
            'Uses a simple EM-like algorithm for training.',
            'Result: performance competitive with supervised methods (4.9 AER on Hansards).'),
  abstract(<<EOF
We present an unsupervised approach to symmetric
word alignment in which two simple asymmetric models are
trained jointly to maximize a
combination of data likelihood and agreement between the models.
Compared to the standard practice of intersecting predictions of
independently-trained models, joint training provides a 32\% reduction
in AER.  Moreover, a simple and efficient pair of HMM aligners
provides a 29\% reduction in AER over symmetrized IBM model 4
predictions.
EOF
  ),
)

entry!('liang05meng',
  author('Percy Liang'),
  title('Semi-Supervised Learning for Natural Language'),
  mastersthesis('Massachusetts Institute of Technology', 2005),
  url('meng-thesis.pdf'),
  errataurl('meng-thesis-errata.pdf'),
  punchlines('Task: named-entity recognition and Chinese word segmentation',
            'Idea: create features based on unlabeled data '+
            'to use in Perceptron learning in Markov or semi-Markov models'),
)

entry!('liang05hypercycle',
  author('Percy Liang and Nathan Srebro'),
  title('A Data Structure for Maintaining Acyclicity in Hypergraphs'),
  mit(2005),
  url('hypercycle-2005.pdf'),
  code('../software/hypertree.zip'),
  punchlines('We introduce the first definition of hyperacyclicity for hypergraphs, '+
            'a generalization of acyclicity in graphs.',
            'We provide a dynamic data structure for maintaining hyperacyclicity, '+
            'a generalization of Tarjan\'s Union-Find algorithm.'),
)

entry!('liang04markov',
  title('Methods and Experiments With Bounded Tree-width {M}arkov Networks'),
  author('Percy Liang and Nathan Srebro'),
  mit(2004),
  url('markov-experiments.pdf'),
  code('../software/hypertree.zip'),
  punchlines('Use a greedy procedure to find the maximum likelihood (or MDL) '+
            'bounded tree-width Markov network '+
            '(for tree-width 1, equivalent to Chow-Liu maximum spanning trees).'),
)

entry!('liang03maxwmfarm',
  title('How Much Of A Hypertree Can Be Captured By Windmills?'),
  author('Percy Liang and Nathan Srebro'),
  url('maxwmfarm.pdf'),
  mit(2003),
  code('../software/hypertree.zip'),
  punchlines('Use linear programming to find worst case inputs to a dynamic program '+
            'in order to explore the tightness of a bound '+
            'for approximating maximum weight hypertrees with windmill farms.'),
)

entry!('liang05mcmaster',
  title('Linear Programming in Bounded Tree-width {M}arkov Networks'),
  author('Percy Liang and Nathan Srebro'),
  inproceedings('Mathematical Programing for Data Mining and Machine Learning Workshop at McMaster University', 2005),
  code('../software/hypertree.zip'),
  slidesurl('mcmaster2005-slides.pdf'),
)

entry!('liang05geometric',
  title('Efficient Geometric Algorithms for Parsing in Two Dimensions'),
  author('Percy Liang and Mukund Narasimhan and Michael Shilman and Paul Viola'),
  inproceedings('International Conference on Document Analysis and Recognition (ICDAR)', 2005),
  url('geometric-parsing-icdar2005.pdf'),
  punchlines('In parsing sequences using dynamic programming, '+
            'the subproblems are continguous subsequences (quadratic in number of terminals). '+
            'In parsing documents or images, '+
            'the subproblems would be subsets of the terminals (exponential in number of terminals). '+
            'We introduce (and unify) several ways to constrain these subsets using the geometric structure of the terminals.'),
)
